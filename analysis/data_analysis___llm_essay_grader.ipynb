{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubgqPx_fuGpo"
   },
   "source": [
    "# Import & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_SOmoCbE-8r",
    "outputId": "33178fb6-3cae-4f54-e568-e6cf7af8e06d"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn openai\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "import re\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IJ9LcyWum1c",
    "outputId": "0e76ab53-7860-4602-a8ea-8fa05584c16d"
   },
   "outputs": [],
   "source": [
    "!pip install fitz\n",
    "!pip install pymupdf\n",
    "!pip install PyPDF2\n",
    "!pip install pingouin\n",
    "\n",
    "import PyPDF2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "API_KEY_SENTINEL=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "YaspGaZ7uJY0",
    "outputId": "aee26f23-1a1c-480d-ced7-bce87f62a5ff"
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# === 1. Load and extract essays from PDF ===\n",
    "pdf_path = \"essay_grading_content/essays_for_grading_Ray Zhou.pdf\"\n",
    "\n",
    "doc = fitz.open(pdf_path)\n",
    "full_text = \"\"\n",
    "for page in doc:\n",
    "    full_text += page.get_text(\"text\") + \"\\n\"\n",
    "\n",
    "# Split essays using \"ID:\" markers\n",
    "essays_raw = re.split(r\"(?=ID: )\", full_text)\n",
    "essays = []\n",
    "for chunk in essays_raw:\n",
    "    if not chunk.strip().startswith(\"ID:\"):\n",
    "        continue\n",
    "\n",
    "    # Extract ID\n",
    "    m_id = re.search(r\"ID:\\s*([a-zA-Z0-9]+)\", chunk)\n",
    "    if not m_id:\n",
    "        continue\n",
    "    essay_id = m_id.group(1).strip()\n",
    "\n",
    "    # Extract prompt\n",
    "    m_prompt = re.search(r\"PROMPT:(.*?)---\", chunk, re.S)\n",
    "    prompt = m_prompt.group(1).strip() if m_prompt else \"\"\n",
    "\n",
    "    # Extract essay text (everything after ---)\n",
    "    m_essay = re.search(r\"---(.*)\", chunk, re.S)\n",
    "    essay_text = m_essay.group(1).strip() if m_essay else \"\"\n",
    "\n",
    "    essays.append({\n",
    "        \"ID\": essay_id,\n",
    "        \"Prompt\": prompt,\n",
    "        \"Essay\": essay_text\n",
    "    })\n",
    "\n",
    "essays_df = pd.DataFrame(essays)\n",
    "print(f\"✅ Parsed {len(essays_df)} essays from PDF\")\n",
    "\n",
    "# === 2. Load grades CSV ===\n",
    "grades_path = \"essay_grading_content/essay_grades.csv\"\n",
    "grades_df = pd.read_csv(grades_path)\n",
    "\n",
    "# Clean column names\n",
    "grades_df.columns = grades_df.columns.str.strip()\n",
    "\n",
    "# Rename Prolific Id → ID\n",
    "if \"Prolific Id\" in grades_df.columns:\n",
    "    grades_df.rename(columns={\"Prolific Id\": \"ID\"}, inplace=True)\n",
    "\n",
    "print(\"✅ Loaded grades file with columns:\", grades_df.columns.tolist())\n",
    "\n",
    "# === 3. Merge essays + grades ===\n",
    "merged_df = pd.merge(essays_df, grades_df, on=\"ID\", how=\"outer\")\n",
    "print(f\"✅ Merged dataset has {len(merged_df)} rows\")\n",
    "\n",
    "# === 4. Preview ===\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uw-CJ_RvOlB",
    "outputId": "69f97deb-1e8a-4a77-ec96-76756f7e194f"
   },
   "outputs": [],
   "source": [
    "rubric_text = \"\"\n",
    "with open(\"essay_grading_content/Band-Descriptors-Task-2.pdf\", \"rb\") as f:  # \"rb\" = read binary\n",
    "    reader = PyPDF2.PdfReader(f)\n",
    "    for page in reader.pages:\n",
    "        rubric_text += page.extract_text() + \"\\n\"\n",
    "\n",
    "print(rubric_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Gf6gu7vxIcK",
    "outputId": "e9cd80bb-d5e7-4d46-abdb-4cbbcd979812"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "examples_df, test_df = train_test_split(merged_df, test_size=20, random_state=42)\n",
    "print(f\"Examples: {len(examples_df)}, Test set: {len(test_df)}\")\n",
    "\n",
    "\n",
    "def make_examples(df, n=30):\n",
    "    examples = \"\"\n",
    "    for _, row in df.head(n).iterrows():  # take first n\n",
    "        examples += f\"\"\"\n",
    "        Essay:\n",
    "        {row['Essay']}\n",
    "\n",
    "        Scores:\n",
    "        - Quality of argument: {row['Quality of argument']}\n",
    "        - Coherence and cohesion: {row['Coherence and cohesion']}\n",
    "        - Lexical resource and grammar: {row['Lexical resource and grammar']}\n",
    "\n",
    "        ---\n",
    "        \"\"\"\n",
    "    return examples\n",
    "\n",
    "examples = make_examples(examples_df, n=30)\n",
    "print(examples[:1500])  # preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_Zd0AoWxNFf"
   },
   "outputs": [],
   "source": [
    "prompt_template = f\"\"\"\n",
    "You are an expert IELTS examiner. Your task is to evaluate student essays according to the IELTS Task 2 rubric.\n",
    "\n",
    "You will give **three separate scores (0–9)**:\n",
    "1. Quality of argument – how well the student develops and supports their ideas.\n",
    "2. Coherence and cohesion – how logically the essay flows and how ideas are connected.\n",
    "3. Lexical resource and grammar – vocabulary range, accuracy, and grammar use.\n",
    "\n",
    "Rubric (truncated to stay within context length):\n",
    "{rubric_text}\n",
    "\n",
    "Here are 30 graded examples for reference:\n",
    "{examples}\n",
    "\n",
    "Now grade the following essay: {{essay_text}}\n",
    "\n",
    "\n",
    "\n",
    "Assign scores that reflect the standards from the examples. Align with the reference grader.\n",
    "\n",
    "1. Quality of argument: Look for weak reasoning, insufficient support, or underdeveloped ideas.\n",
    "   Avoid giving “safe” midrange scores unless they truly match the examples. Align with the reference grader.\n",
    "\n",
    "2. Coherence and cohesion: Penalize logical gaps, poor transitions, or unclear organization.\n",
    "   Align with the reference grader.\n",
    "\n",
    "3. Lexical resource and grammar: Evaluate normally based on the rubric.\n",
    "   Align with the reference grader.\n",
    "\n",
    "\n",
    "Output format:\n",
    "Quality of argument: X\n",
    "Coherence and cohesion: Y\n",
    "Lexical resource and grammar: Z\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "import os, re\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY_SENTINEL  # keep safe!\n",
    "client = OpenAI()\n",
    "\n",
    "def grade_essay(essay_text):\n",
    "    prompt = prompt_template.format(essay_text=essay_text)\n",
    "    resp = client.responses.create(\n",
    "    model=\"o4-mini\",\n",
    "    reasoning={\"effort\": \"high\"},\n",
    "    input=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"input_text\", \"text\": prompt}],\n",
    "        }],\n",
    "    )\n",
    "    return resp.output_text\n",
    "\n",
    "def extract_scores(text):\n",
    "    # extract first 3 numbers (0–9)\n",
    "    m = re.findall(r\"\\b([0-9])\\b\", text)\n",
    "    if len(m) >= 3:\n",
    "        return list(map(int, m[:3]))\n",
    "    return [None, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6na4ZYRDadc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-l_ymWNwData"
   },
   "source": [
    "# Compare to Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "XsmIgkcDxZAU",
    "outputId": "159b4129-da0d-479d-ffbe-8ce2eb5c3997"
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#################################################################################\n",
    "################# ########.   Grade Here    .######## ###########################\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "\n",
    "results = []\n",
    "for _, row in test_df.iterrows():\n",
    "    gpt_output = grade_essay(row[\"Essay\"])\n",
    "    gpt_scores = extract_scores(gpt_output)\n",
    "    results.append({\n",
    "        \"ID\": row[\"ID\"],\n",
    "        \"Human\": [\n",
    "            row[\"Quality of argument\"],\n",
    "            row[\"Coherence and cohesion\"],\n",
    "            row[\"Lexical resource and grammar\"]\n",
    "        ],\n",
    "        \"GPT\": gpt_scores,\n",
    "        \"Raw\": gpt_output\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "owpwIJEv4R4i",
    "outputId": "7e1bebec-80c8-4ce2-e87a-e74af05c7236"
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zM8ztRnPqyCz"
   },
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV\n",
    "results_df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6Wvm1KOZEwJd",
    "outputId": "35f317f0-8e2e-44e4-a870-234189537d38"
   },
   "outputs": [],
   "source": [
    "result_df = results_df\n",
    "print(\"DataFrame info:\")\n",
    "print(f\"Shape: {result_df.shape}\")\n",
    "print(\"\\nColumns:\", result_df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(result_df.head())\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Expand Human and GPT scores into separate columns\n",
    "results_df[[\"Human_Arg\", \"Human_Coh\", \"Human_Lex\"]] = pd.DataFrame(results_df[\"Human\"].tolist(), index=results_df.index)\n",
    "results_df[[\"GPT_Arg\", \"GPT_Coh\", \"GPT_Lex\"]] = pd.DataFrame(results_df[\"GPT\"].tolist(), index=results_df.index)\n",
    "\n",
    "# Compute differences\n",
    "results_df[\"Diff_Arg\"] = results_df[\"GPT_Arg\"] - results_df[\"Human_Arg\"]\n",
    "results_df[\"Diff_Coh\"] = results_df[\"GPT_Coh\"] - results_df[\"Human_Coh\"]\n",
    "results_df[\"Diff_Lex\"] = results_df[\"GPT_Lex\"] - results_df[\"Human_Lex\"]\n",
    "\n",
    "# Absolute differences\n",
    "results_df[\"AbsDiff_Arg\"] = results_df[\"Diff_Arg\"].abs()\n",
    "results_df[\"AbsDiff_Coh\"] = results_df[\"Diff_Coh\"].abs()\n",
    "results_df[\"AbsDiff_Lex\"] = results_df[\"Diff_Lex\"].abs()\n",
    "\n",
    "# Summary metrics\n",
    "summary = {\n",
    "    \"Mean Abs Diff (Arg)\": results_df[\"AbsDiff_Arg\"].mean(),\n",
    "    \"Mean Abs Diff (Coh)\": results_df[\"AbsDiff_Coh\"].mean(),\n",
    "    \"Mean Abs Diff (Lex)\": results_df[\"AbsDiff_Lex\"].mean(),\n",
    "    \"Overall Mean Abs Diff\": results_df[[\"AbsDiff_Arg\",\"AbsDiff_Coh\",\"AbsDiff_Lex\"]].mean().mean()\n",
    "}\n",
    "\n",
    "print(\"=== Score Differences Summary ===\")\n",
    "for k,v in summary.items():\n",
    "    print(f\"{k}: {v:.2f}\")\n",
    "\n",
    "# Show sample rows\n",
    "print(\"\\n=== Sample Comparison ===\")\n",
    "print(results_df[[\"ID\",\"Human_Arg\",\"GPT_Arg\",\"Diff_Arg\",\n",
    "                 \"Human_Coh\",\"GPT_Coh\",\"Diff_Coh\",\n",
    "                 \"Human_Lex\",\"GPT_Lex\",\"Diff_Lex\"]].head(10))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pingouin as pg\n",
    "\n",
    "# === 1. Scatterplots ===\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "dims = [(\"Human_Arg\",\"GPT_Arg\",\"Quality of Argument\"),\n",
    "        (\"Human_Coh\",\"GPT_Coh\",\"Coherence & Cohesion\"),\n",
    "        (\"Human_Lex\",\"GPT_Lex\",\"Lexical & Grammar\")]\n",
    "\n",
    "for ax,(h,g,label) in zip(axes,dims):\n",
    "    sns.scatterplot(x=results_df[h], y=results_df[g], ax=ax)\n",
    "    ax.plot([0,9],[0,9], 'r--')  # 1:1 line\n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel(\"Human\")\n",
    "    ax.set_ylabel(\"GPT\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 2. Correlations ===\n",
    "for h,g,label in dims:\n",
    "    pearson = pearsonr(results_df[h], results_df[g])\n",
    "    spearman = spearmanr(results_df[h], results_df[g])\n",
    "    print(f\"\\n{label}\")\n",
    "    print(f\"  Pearson r: {pearson[0]:.3f} (p={pearson[1]:.3g})\")\n",
    "    print(f\"  Spearman rho: {spearman[0]:.3f} (p={spearman[1]:.3g})\")\n",
    "\n",
    "# === 3. Intraclass Correlation Coefficient (ICC) ===\n",
    "# Reshape into long form: ID, Rater, Score, Dimension\n",
    "icc_data = []\n",
    "for idx,row in results_df.iterrows():\n",
    "    icc_data.append([row[\"ID\"], \"Human\", row[\"Human_Arg\"], \"Argument\"])\n",
    "    icc_data.append([row[\"ID\"], \"GPT\",   row[\"GPT_Arg\"], \"Argument\"])\n",
    "    icc_data.append([row[\"ID\"], \"Human\", row[\"Human_Coh\"], \"Coherence\"])\n",
    "    icc_data.append([row[\"ID\"], \"GPT\",   row[\"GPT_Coh\"], \"Coherence\"])\n",
    "    icc_data.append([row[\"ID\"], \"Human\", row[\"Human_Lex\"], \"Lexical\"])\n",
    "    icc_data.append([row[\"ID\"], \"GPT\",   row[\"GPT_Lex\"], \"Lexical\"])\n",
    "\n",
    "icc_df = pd.DataFrame(icc_data, columns=[\"ID\",\"Rater\",\"Score\",\"Dimension\"])\n",
    "\n",
    "# Run ICC separately for each dimension\n",
    "for dim in icc_df[\"Dimension\"].unique():\n",
    "    sub = icc_df[icc_df[\"Dimension\"]==dim]\n",
    "    icc = pg.intraclass_corr(data=sub, targets=\"ID\", raters=\"Rater\", ratings=\"Score\")\n",
    "    print(f\"\\nICC for {dim}:\\n\", icc[[\"Type\",\"ICC\",\"CI95%\",\"pval\"]])\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Unpack the scores from lists into separate columns\n",
    "results_df[\"Human_Arg\"]     = results_df[\"Human\"].apply(lambda x: x[0])\n",
    "results_df[\"Human_Coh\"]     = results_df[\"Human\"].apply(lambda x: x[1])\n",
    "results_df[\"Human_Lex\"]     = results_df[\"Human\"].apply(lambda x: x[2])\n",
    "\n",
    "results_df[\"GPT_Arg\"]       = results_df[\"GPT\"].apply(lambda x: x[0])\n",
    "results_df[\"GPT_Coh\"]       = results_df[\"GPT\"].apply(lambda x: x[1])\n",
    "results_df[\"GPT_Lex\"]       = results_df[\"GPT\"].apply(lambda x: x[2])\n",
    "\n",
    "# Set up figure\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "dims = [(\"Human_Arg\",\"GPT_Arg\",\"Quality of Argument\"),\n",
    "        (\"Human_Coh\",\"GPT_Coh\",\"Coherence & Cohesion\"),\n",
    "        (\"Human_Lex\",\"GPT_Lex\",\"Lexical & Grammar\")]\n",
    "\n",
    "for ax,(h,g,label) in zip(axes,dims):\n",
    "    sns.scatterplot(x=results_df[h], y=results_df[g], ax=ax)\n",
    "    ax.plot([0,9],[0,9], 'r--')  # 1:1 reference line\n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel(\"Human Score\")\n",
    "    ax.set_ylabel(\"GPT Score\")\n",
    "    ax.set_xticks(range(0,10))\n",
    "    ax.set_yticks(range(0,10))\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Cohen's kappa ===\n",
    "print(\"Cohen's Kappa:\")\n",
    "print(\"  Argument:\", cohen_kappa_score(results_df[\"Human_Arg\"], results_df[\"GPT_Arg\"]))\n",
    "print(\"  Cohesion:\", cohen_kappa_score(results_df[\"Human_Coh\"], results_df[\"GPT_Coh\"]))\n",
    "print(\"  Lexical :\", cohen_kappa_score(results_df[\"Human_Lex\"], results_df[\"GPT_Lex\"]))\n",
    "\n",
    "# === Confusion Matrix Heatmaps ===\n",
    "dims = [(\"Human_Arg\",\"GPT_Arg\",\"Quality of Argument\"),\n",
    "        (\"Human_Coh\",\"GPT_Coh\",\"Coherence & Cohesion\"),\n",
    "        (\"Human_Lex\",\"GPT_Lex\",\"Lexical & Grammar\")]\n",
    "\n",
    "for h,g,label in dims:\n",
    "    cm = confusion_matrix(results_df[h], results_df[g], labels=range(0,10))\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.title(f\"Confusion Matrix: {label}\")\n",
    "    plt.xlabel(\"GPT Score\")\n",
    "    plt.ylabel(\"Human Score\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Extract each dimension into lists\n",
    "human_arg     = results_df[\"Human\"].apply(lambda x: x[0])\n",
    "human_cohes   = results_df[\"Human\"].apply(lambda x: x[1])\n",
    "human_lexical = results_df[\"Human\"].apply(lambda x: x[2])\n",
    "\n",
    "gpt_arg     = results_df[\"GPT\"].apply(lambda x: x[0])\n",
    "gpt_cohes   = results_df[\"GPT\"].apply(lambda x: x[1])\n",
    "gpt_lexical = results_df[\"GPT\"].apply(lambda x: x[2])\n",
    "\n",
    "# Weighted kappas\n",
    "print(\"Weighted Kappa (quadratic):\")\n",
    "print(\"  Argument:\", cohen_kappa_score(human_arg, gpt_arg, weights=\"quadratic\"))\n",
    "print(\"  Cohesion:\", cohen_kappa_score(human_cohes, gpt_cohes, weights=\"quadratic\"))\n",
    "print(\"  Lexical :\", cohen_kappa_score(human_lexical, gpt_lexical, weights=\"quadratic\"))\n",
    "\n",
    "print(\"\\nWeighted Kappa (linear):\")\n",
    "print(\"  Argument:\", cohen_kappa_score(human_arg, gpt_arg, weights=\"linear\"))\n",
    "print(\"  Cohesion:\", cohen_kappa_score(human_cohes, gpt_cohes, weights=\"linear\"))\n",
    "print(\"  Lexical :\", cohen_kappa_score(human_lexical, gpt_lexical, weights=\"linear\"))\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Cohen's kappa ===\n",
    "print(\"Cohen's Kappa:\")\n",
    "print(\"  Argument:\", cohen_kappa_score(results_df[\"Human_Arg\"], results_df[\"GPT_Arg\"]))\n",
    "print(\"  Cohesion:\", cohen_kappa_score(results_df[\"Human_Coh\"], results_df[\"GPT_Coh\"]))\n",
    "print(\"  Lexical :\", cohen_kappa_score(results_df[\"Human_Lex\"], results_df[\"GPT_Lex\"]))\n",
    "\n",
    "# === Confusion Matrix Heatmaps ===\n",
    "dims = [(\"Human_Arg\",\"GPT_Arg\",\"Quality of Argument\"),\n",
    "        (\"Human_Coh\",\"GPT_Coh\",\"Coherence & Cohesion\"),\n",
    "        (\"Human_Lex\",\"GPT_Lex\",\"Lexical & Grammar\")]\n",
    "\n",
    "for h,g,label in dims:\n",
    "    cm = confusion_matrix(results_df[h], results_df[g], labels=range(0,10))\n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.title(f\"Confusion Matrix: {label}\")\n",
    "    plt.xlabel(\"GPT Score\")\n",
    "    plt.ylabel(\"Human Score\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Extract each dimension into lists\n",
    "human_arg     = results_df[\"Human\"].apply(lambda x: x[0])\n",
    "human_cohes   = results_df[\"Human\"].apply(lambda x: x[1])\n",
    "human_lexical = results_df[\"Human\"].apply(lambda x: x[2])\n",
    "\n",
    "gpt_arg     = results_df[\"GPT\"].apply(lambda x: x[0])\n",
    "gpt_cohes   = results_df[\"GPT\"].apply(lambda x: x[1])\n",
    "gpt_lexical = results_df[\"GPT\"].apply(lambda x: x[2])\n",
    "\n",
    "# Weighted kappas\n",
    "print(\"Weighted Kappa (quadratic):\")\n",
    "print(\"  Argument:\", cohen_kappa_score(human_arg, gpt_arg, weights=\"quadratic\"))\n",
    "print(\"  Cohesion:\", cohen_kappa_score(human_cohes, gpt_cohes, weights=\"quadratic\"))\n",
    "print(\"  Lexical :\", cohen_kappa_score(human_lexical, gpt_lexical, weights=\"quadratic\"))\n",
    "\n",
    "print(\"\\nWeighted Kappa (linear):\")\n",
    "print(\"  Argument:\", cohen_kappa_score(human_arg, gpt_arg, weights=\"linear\"))\n",
    "print(\"  Cohesion:\", cohen_kappa_score(human_cohes, gpt_cohes, weights=\"linear\"))\n",
    "print(\"  Lexical :\", cohen_kappa_score(human_lexical, gpt_lexical, weights=\"linear\"))\n",
    "\n",
    "\n",
    "\n",
    "# Calculate absolute differences if not already present\n",
    "if 'AbsDiff_Arg' not in result_df.columns:\n",
    "    result_df['AbsDiff_Arg'] = abs(result_df['Diff_Arg'])\n",
    "    result_df['AbsDiff_Coh'] = abs(result_df['Diff_Coh'])\n",
    "    result_df['AbsDiff_Lex'] = abs(result_df['Diff_Lex'])\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "categories = ['Arg', 'Coh', 'Lex']\n",
    "category_names = ['Quality of Argument', 'Coherence & Cohesion', 'Lexical Resource & Grammar']\n",
    "\n",
    "for i, cat in enumerate(categories):\n",
    "    human_col = f'Human_{cat}'\n",
    "    gpt_col = f'GPT_{cat}'\n",
    "    diff_col = f'Diff_{cat}'\n",
    "    abs_diff_col = f'AbsDiff_{cat}'\n",
    "\n",
    "    print(f\"\\n{category_names[i]}:\")\n",
    "    print(f\"  Human - Mean: {result_df[human_col].mean():.2f}, Std: {result_df[human_col].std():.2f}, Range: {result_df[human_col].min()}-{result_df[human_col].max()}\")\n",
    "    print(f\"  GPT   - Mean: {result_df[gpt_col].mean():.2f}, Std: {result_df[gpt_col].std():.2f}, Range: {result_df[gpt_col].min()}-{result_df[gpt_col].max()}\")\n",
    "    print(f\"  Mean difference (Human-GPT): {result_df[diff_col].mean():.2f}\")\n",
    "    print(f\"  Mean absolute difference: {result_df[abs_diff_col].mean():.2f}\")\n",
    "    print(f\"  Correlation: {result_df[human_col].corr(result_df[gpt_col]):.3f}\")\n",
    "\n",
    "# Function to calculate agreement with tolerance\n",
    "def calculate_agreement_with_tolerance(human_scores, gpt_scores, tolerance=0):\n",
    "    \"\"\"Calculate agreement rate with tolerance for scoring differences\"\"\"\n",
    "    agreement = abs(np.array(human_scores) - np.array(gpt_scores)) <= tolerance\n",
    "    return np.mean(agreement)\n",
    "\n",
    "# Function to calculate weighted kappa with tolerance\n",
    "def calculate_weighted_kappa_tolerance(human_scores, gpt_scores, tolerance=0):\n",
    "    \"\"\"Calculate Cohen's kappa considering tolerance\"\"\"\n",
    "    if tolerance == 0:\n",
    "        return cohen_kappa_score(human_scores, gpt_scores)\n",
    "\n",
    "    # For tolerance > 0, we'll calculate agreement-based kappa\n",
    "    agreement_rate = calculate_agreement_with_tolerance(human_scores, gpt_scores, tolerance)\n",
    "\n",
    "    # Simple kappa approximation based on agreement\n",
    "    # This is a simplified version - exact calculation would require contingency tables\n",
    "    num_categories = len(np.unique(np.concatenate([human_scores, gpt_scores])))\n",
    "    expected_agreement = 1 / num_categories\n",
    "\n",
    "    if expected_agreement >= 1.0:\n",
    "        return 1.0 if agreement_rate == 1.0 else 0.0\n",
    "\n",
    "    kappa = (agreement_rate - expected_agreement) / (1 - expected_agreement)\n",
    "    return max(-1, min(1, kappa))  # Bound between -1 and 1\n",
    "\n",
    "# Calculate Cohen's Kappa for different tolerance levels\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COHEN'S KAPPA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tolerance_levels = [0, 1, 2, 3]\n",
    "kappa_results = {}\n",
    "\n",
    "for i, cat in enumerate(categories):\n",
    "    human_scores = result_df[f'Human_{cat}'].values\n",
    "    gpt_scores = result_df[f'GPT_{cat}'].values\n",
    "\n",
    "    print(f\"\\n{category_names[i]}:\")\n",
    "    kappa_results[cat] = {}\n",
    "\n",
    "    for tol in tolerance_levels:\n",
    "        if tol == 0:\n",
    "            kappa = cohen_kappa_score(human_scores, gpt_scores)\n",
    "            agreement = calculate_agreement_with_tolerance(human_scores, gpt_scores, tol)\n",
    "        else:\n",
    "            kappa = calculate_weighted_kappa_tolerance(human_scores, gpt_scores, tol)\n",
    "            agreement = calculate_agreement_with_tolerance(human_scores, gpt_scores, tol)\n",
    "\n",
    "        print(f\"  Tolerance ±{tol}: κ = {kappa:.3f}, Agreement = {agreement:.3f} ({agreement*100:.1f}%)\")\n",
    "        kappa_results[cat][tol] = {'kappa': kappa, 'agreement': agreement}\n",
    "\n",
    "# Interpretation of kappa values\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"KAPPA INTERPRETATION:\")\n",
    "print(\"=\"*50)\n",
    "print(\"< 0.00: Poor agreement\")\n",
    "print(\"0.00-0.20: Slight agreement\")\n",
    "print(\"0.21-0.40: Fair agreement\")\n",
    "print(\"0.41-0.60: Moderate agreement\")\n",
    "print(\"0.61-0.80: Substantial agreement\")\n",
    "print(\"0.81-1.00: Almost perfect agreement\")\n",
    "\n",
    "# Create visualizations\n",
    "plt.style.use('default')\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Create a 3x3 grid for better layout\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "# Scatter plots comparing Human vs GPT scores (top row)\n",
    "for i, cat in enumerate(categories):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    human_scores = result_df[f'Human_{cat}']\n",
    "    gpt_scores = result_df[f'GPT_{cat}']\n",
    "\n",
    "    ax.scatter(human_scores, gpt_scores, alpha=0.7, s=100, color=colors[i])\n",
    "\n",
    "    # Add diagonal line (perfect agreement)\n",
    "    min_score = min(human_scores.min(), gpt_scores.min())\n",
    "    max_score = max(human_scores.max(), gpt_scores.max())\n",
    "    ax.plot([min_score, max_score], [min_score, max_score], 'k--', alpha=0.5, label='Perfect Agreement')\n",
    "\n",
    "    # Add trend line\n",
    "    z = np.polyfit(human_scores, gpt_scores, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(human_scores, p(human_scores), \"r-\", alpha=0.7, label='Trend')\n",
    "\n",
    "    correlation = np.corrcoef(human_scores, gpt_scores)[0,1]\n",
    "    ax.set_xlabel('Human Scores', fontsize=12)\n",
    "    ax.set_ylabel('GPT Scores', fontsize=12)\n",
    "    ax.set_title(f'{category_names[i]}\\n(r = {correlation:.3f})', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "# Distribution plots (middle row)\n",
    "for i, cat in enumerate(categories):\n",
    "    ax = fig.add_subplot(gs[1, i])\n",
    "\n",
    "    human_scores = result_df[f'Human_{cat}']\n",
    "    gpt_scores = result_df[f'GPT_{cat}']\n",
    "\n",
    "    # Create bins based on actual score range\n",
    "    min_score = min(human_scores.min(), gpt_scores.min())\n",
    "    max_score = max(human_scores.max(), gpt_scores.max())\n",
    "    bins = np.arange(min_score, max_score + 2) - 0.5\n",
    "\n",
    "    ax.hist(human_scores, bins=bins, alpha=0.6, label='Human', color=colors[i], density=True)\n",
    "    ax.hist(gpt_scores, bins=bins, alpha=0.6, label='GPT', color='orange', density=True)\n",
    "\n",
    "    ax.set_xlabel('Scores', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title(f'{category_names[i]} Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Agreement by tolerance (bottom row - spans all columns)\n",
    "ax = fig.add_subplot(gs[2, :])\n",
    "\n",
    "tolerance_range = range(len(tolerance_levels))\n",
    "width = 0.25\n",
    "\n",
    "for i, cat in enumerate(categories):\n",
    "    agreements = [kappa_results[cat][tol]['agreement'] for tol in tolerance_levels]\n",
    "    x_pos = np.array(tolerance_range) + i * width\n",
    "    ax.bar(x_pos, agreements, width, label=f'{category_names[i]}', color=colors[i], alpha=0.7)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for j, (x, agreement) in enumerate(zip(x_pos, agreements)):\n",
    "        ax.text(x, agreement + 0.01, f'{agreement:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Tolerance Level (±points)', fontsize=14)\n",
    "ax.set_ylabel('Agreement Rate', fontsize=14)\n",
    "ax.set_title('Agreement Rate by Tolerance Level', fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(np.array(tolerance_range) + width)\n",
    "ax.set_xticklabels([f'±{t}' for t in tolerance_levels])\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.suptitle('Human vs GPT Scoring Analysis', fontsize=20, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Statistical tests\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, cat in enumerate(categories):\n",
    "    human_scores = result_df[f'Human_{cat}']\n",
    "    gpt_scores = result_df[f'GPT_{cat}']\n",
    "\n",
    "    # Paired t-test\n",
    "    t_stat, p_value = stats.ttest_rel(human_scores, gpt_scores)\n",
    "\n",
    "    # Wilcoxon signed-rank test (non-parametric alternative)\n",
    "    try:\n",
    "        w_stat, w_p_value = stats.wilcoxon(human_scores, gpt_scores, zero_method='wilcox')\n",
    "        wilcoxon_result = f\"W = {w_stat:.1f}, p = {w_p_value:.4f}\"\n",
    "    except ValueError:\n",
    "        wilcoxon_result = \"Cannot compute (identical distributions)\"\n",
    "\n",
    "    print(f\"\\n{category_names[i]}:\")\n",
    "    print(f\"  Paired t-test: t = {t_stat:.3f}, p = {p_value:.4f}\")\n",
    "    print(f\"  Wilcoxon test: {wilcoxon_result}\")\n",
    "\n",
    "    if p_value < 0.001:\n",
    "        print(\"  → Highly significant difference (p < 0.001)\")\n",
    "    elif p_value < 0.01:\n",
    "        print(\"  → Very significant difference (p < 0.01)\")\n",
    "    elif p_value < 0.05:\n",
    "        print(\"  → Significant difference (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"  → No significant difference (p ≥ 0.05)\")\n",
    "\n",
    "# Detailed breakdown by score differences\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCORE DIFFERENCE BREAKDOWN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, cat in enumerate(categories):\n",
    "    diff_col = f'Diff_{cat}'\n",
    "    print(f\"\\n{category_names[i]} - Score Differences (Human - GPT):\")\n",
    "\n",
    "    diff_counts = result_df[diff_col].value_counts().sort_index()\n",
    "    total = len(result_df)\n",
    "\n",
    "    for diff_val, count in diff_counts.items():\n",
    "        percentage = (count / total) * 100\n",
    "        direction = \"Human higher\" if diff_val > 0 else \"GPT higher\" if diff_val < 0 else \"Equal\"\n",
    "        print(f\"  Difference {diff_val:+2d}: {count:2d} cases ({percentage:5.1f}%) - {direction}\")\n",
    "\n",
    "# Create summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KAPPA SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "for i, cat in enumerate(categories):\n",
    "    for tol in tolerance_levels:\n",
    "        summary_data.append({\n",
    "            'Category': category_names[i],\n",
    "            'Tolerance': f'±{tol}',\n",
    "            'Kappa': f\"{kappa_results[cat][tol]['kappa']:.3f}\",\n",
    "            'Agreement': f\"{kappa_results[cat][tol]['agreement']:.3f}\",\n",
    "            'Agreement %': f\"{kappa_results[cat][tol]['agreement']*100:.1f}%\"\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Confusion matrices for exact agreement (tolerance = 0)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRICES (Exact Agreement)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, cat in enumerate(categories):\n",
    "    human_scores = result_df[f'Human_{cat}']\n",
    "    gpt_scores = result_df[f'GPT_{cat}']\n",
    "\n",
    "    print(f\"\\n{category_names[i]}:\")\n",
    "    cm = confusion_matrix(human_scores, gpt_scores)\n",
    "\n",
    "    # Get unique scores for labels\n",
    "    unique_scores = sorted(set(human_scores) | set(gpt_scores))\n",
    "\n",
    "    # Create a formatted confusion matrix\n",
    "    cm_df = pd.DataFrame(cm, index=[f'H_{s}' for s in unique_scores],\n",
    "                        columns=[f'G_{s}' for s in unique_scores])\n",
    "    print(cm_df)\n",
    "\n",
    "print(f\"\\n\\nAnalysis complete! Dataset contains {len(result_df)} essays.\")\n",
    "print(\"Key findings will be in the statistical tests and kappa values above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGSdwmjuEB1t"
   },
   "source": [
    "# Grade Real Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import os\n",
    "\n",
    "# build URL from the locally‑forwarded port\n",
    "user     = os.getenv(\"DB_USER\")\n",
    "pw       = os.getenv(\"DB_PASSWORD\")\n",
    "host     = os.getenv(\"DB_HOST\")\n",
    "port     = os.getenv(\"DB_PORT\")\n",
    "db       = os.getenv(\"DB_NAME\")\n",
    "engine   = create_engine(f\"postgresql://{user}:{pw}@{host}:{port}/{db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = pd.read_sql(\"SELECT * FROM text_snapshots;\", engine)\n",
    "snapshots.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of accepted participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_list = []\n",
    "\n",
    "with open(\"pid_accepted.txt\", \"r\") as fle:\n",
    "    for line in fle:\n",
    "        pid_list.append(line.strip())\n",
    "        \n",
    "len(pid_list), pid_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get final essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_essay(pid):\n",
    "    filtered = snapshots.loc[\n",
    "        (snapshots[\"participant_id\"] == pid) & \n",
    "        (snapshots[\"type\"] == \"final\") &\n",
    "        (snapshots[\"stage\"] == \"revision\")\n",
    "    ]\n",
    "\n",
    "    if len(filtered) > 1:\n",
    "        print(f\"warning [{pid}]: more than one final submission\")\n",
    "        filtered = filtered.drop_duplicates(subset=\"participant_id\", keep=\"last\")\n",
    "    \n",
    "    return filtered.iloc[0][\"text_content\"]\n",
    "\n",
    "get_final_essay(\"63e584009bf1aa55a39e1d53\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DNQTvl7RCjiD",
    "outputId": "dfed15e0-47fd-48bc-bbb6-8e7f3526b6e4"
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "#################################################################################\n",
    "################# #  Import Essasys to Get Grades   # ###########################\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "fake_data = \"\"\"essay_id,essay_text,human_band\n",
    "1,\"In modern society, technology plays an essential role. While some argue it isolates people, I believe it connects us more deeply by enabling communication across cultures.\",7\n",
    "2,\"The graph shows cars go up. People buy more cars because economy is better. This is good.\",5\n",
    "3,\"Some people think children should learn music at school, while others believe it wastes time. In my opinion, music education develops creativity and discipline that benefit students beyond the classroom.\",8\n",
    "4,\"Nowadays, pollution is big problem. Government must do something fast to stop smoke. If not, people sick.\",4\n",
    "5,\"It is often argued whether university education should be free. I strongly agree because access to education promotes equality and social progress.\",7\n",
    "6,\"People are different in cities. More building, more traffic, more jobs. But village is quiet and good life.\",5\n",
    "7,\"Although many believe social media harms young people, I contend it can foster valuable networks and learning opportunities if used responsibly.\",8\n",
    "8,\"Internet is useful. Student use it. Sometime problem is addiction. That bad.\",4\n",
    "9,\"Some claim space exploration wastes resources. However, it pushes science forward and inspires humanity to solve problems on Earth.\",7\n",
    "10,\"The economy is important. People need job. If no job, then bad. Government help job is good.\",3\n",
    "\"\"\"\n",
    "\n",
    "with open(\"essays_with_scores.csv\", \"w\") as f:\n",
    "    f.write(fake_data)\n",
    "\n",
    "essays_not_from_the_sets = pd.read_csv(\"essays_with_scores.csv\", encoding=\"utf-8-sig\")\n",
    "essays_not_from_the_sets.head()\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "################# ########.   Grade Here    .######## ###########################\n",
    "#################################################################################\n",
    "#################################################################################\n",
    "\n",
    "results = []\n",
    "count = 0\n",
    "\n",
    "for pid in pid_list:\n",
    "    count += 1\n",
    "    print(f\"[{count}] pid: {pid}\")\n",
    "    essay_text = get_final_essay(pid)\n",
    "\n",
    "    # Call your grading function\n",
    "    graded_text = grade_essay(essay_text)\n",
    "\n",
    "    # Extract the three scores\n",
    "    def extract_scores(text):\n",
    "        m = re.findall(r\"\\b([0-9])\\b\", text)\n",
    "        if len(m) >= 3:\n",
    "            return list(map(int, m[:3]))\n",
    "        return [None, None, None]\n",
    "\n",
    "    quality, cohesion, grammar = extract_scores(graded_text)\n",
    "\n",
    "    results.append({\n",
    "        \"participant_id\": pid,\n",
    "        \"essay_text\": essay_text,\n",
    "        \"Quality_of_argument\": quality,\n",
    "        \"Coherence_and_cohesion\": cohesion,\n",
    "        \"Lexical_resource_and_grammar\": grammar\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('csv_exports/graded_essays_with_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_results_df = results_df.drop(\"essay_text\", axis=1)\n",
    "slim_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_results_df.to_csv('csv_exports/graded_essays.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_pickle('essay_grading_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "2kdzIWeDuHmz"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
